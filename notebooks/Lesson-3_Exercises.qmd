---
title: "Tema 3: Ejercicios"
format:
  html:
    code-copy:       true
    code-tools:      true
    embed-resources: true
    theme:           ../www/extra-styles.scss
    toc:             true
    toc-location:    left
callout-appearance: minimal
---

# Introducción

En este hemos visto los fundamentos del modelado Bayesiano, y vamos a aplicarlos desde un punto de vista teórico en los ejercicios a continuación.

En primer lugar, configuramos el entorno para ejecutar el código.

```{r setup}
#| message: false

# Paquetes:
library(tidyverse)
library(RColorBrewer)


# Configuración de la salida gráfica:

PALETA <- brewer.pal(8, "Set2") # Colores por defecto
color_defecto  <- PALETA[1]
options(ggplot2.discrete.colour = PALETA)

theme_set(theme_bw()) # Tema "neutro" para la representación gráfica
```

Verás que solamente necesitamos el paquete {tidyverse}, para manipular datos, y configurar la salida gráfica (el paquete {RColorBrewer} sólo se utiliza para obtener una paleta de colores agradable y accesible para personas con ceguera al color).
No hace falta ningún paquete para análisis y modelado Bayesiano, ya que los modelos que vamos a estimar se basan en las propiedades analíticas de las *distribuciones conjugadas*.

# Ejercicio 1

## Distribución uniforme

A continuación se muestra el código en R para representar la distribución uniforme $x \sim U(0, 1)$:

```{r ejemplo-uniforme}
PREC     <- 1e-3 # Precisión para representar la función de densidad (milésimas)
DENS_INF <- 0    # Rango inferior de la función de densidad
DENS_SUP <- 1    # Rango superior de la función de densidad

uniforme <- tibble( # Esta función crea un "data.frame" o tabla de datos
  variable = seq(from = DENS_INF, to = DENS_SUP, by = PREC),
  densidad = variable |> dunif(min = DENS_INF, max = DENS_SUP)
)

uniforme |> glimpse() # Muestra el objeto con los datos, contiene 2 columnas 

uniforme |> # Usando la tabla de datos antes creada, crea un objeto gráfico
  ggplot(mapping = aes(x = variable, y = densidad)) + # "Mapea" columnas a
                                                      #   coordenadas
  geom_line(color = color_defecto) + # Representa mediante una línea continua
  
  ylim( # Fija el límite inferior a 0 para mostrar el eje y completo:
    0,  # (Usa la propia distribución para establecer el límite superior)
    uniforme |> pull(densidad) |> max()
  )
```

## Distribución normal

Aplicando un código similar, se puede representar una distribución normal estandarizada $x \sim N(0, 1)$:

```{r ejemplo-normal}
DENS_INF <- -4 # Usamos un rango más adecuado para la normal estandarizada
DENS_SUP <-  4

normal <- tibble( # Reutilizamos `PREC` del "chunk" de código anterior
  variable = seq(from = DENS_INF, to = DENS_SUP, by = PREC),
  densidad = variable |> dnorm()
)

# Al cubrir la distribución el rango desde 0 hasta el máximo, en este caso no
#   es necesario establecer los límites manualmente
normal |>
  ggplot(mapping = aes(x = variable, y = densidad)) +
  geom_line(color = color_defecto)
```

Como puedes ver, los límites se establecen automáticamente para cubrir todo el rango de la distribución (no hace falta fijarlos).
Al haber valores próximos a 0, tampoco es necesario establecer el límite inferior manualmente.

## Integración "numérica"

Haciendo uso de los valores generados de una distribución, podemos operar con ellos para obtener los resultados de "integrar" esa función, pero haciéndolo de forma numérica.

Al obtener "valores equiespaciados" de la distribución, lo que estamos obteniendo es una "rejilla" de valores.
La integración será una suma de "rectángulos", de altura igual a la densidad en ese punto, con base centrada en ese punto y extenciéndose `PREC/2` hacia cada lado (y por tanto de anchura `PREC`).

Utilizando esta "integral numérica", podemos obtener ciertos valores de la distribución.
Por ejemplo, la integral en todo el dominio de la variable debería tener un valor de 1.

```{r integral-uniforme}
uniforme |> summarize(integral = PREC * sum(densidad))
```

En el caso de la distribución uniforme, tenemos valores "centrados" en 0 y 1, por lo que los intervalos de los extremos se extienden hasta `-PREC/2` y `1 + PREC/2`.
Podríamos "restar medio valor" de la densidad en cada extremo para obtener una integral más precisa:

```{r}
uniforme |> summarize(
  integral = PREC * (sum(densidad) - 0.5 * (first(densidad) + last(densidad)))
)
```

En el caso de la distribución normal el cálculo de la integral se haría igual:

```{r integral-normal}
normal |> summarize(
  integral = sum(densidad) * PREC
)
```

En este caso, el dominio es infinito, pero nos hemos restringido al rango $[`{r} DENS_INF`, `{r} DENS_SUP`]$.
Por lo tanto, estamos desechando la parte de la distribución que está en las "colas".
También, cuanto mayor sea la precisión, más se acercará la aproximación mediante "rectángulos" a la curva real.

```{r integral-normal-mas-precisa}
tibble( # Ampliando el rango a [-10, 10]:
  variable = seq(from = -10, to = 10, by = PREC),
  densidad = variable |> dnorm()
) |>
  summarize(integral = sum(densidad) * PREC)

tibble( # Usando precisión de "millonésimas":
  variable = seq(from = DENS_INF, to = DENS_SUP, by = 1e-6),
  densidad = variable |> dnorm()
) |>
  summarize(integral = sum(densidad) * 1e-6) # Misma precisión en la integral
```

En general, las aproximaciones iniciales pueden ser válidas.
Si lo necesitamos, podemos "normalizar" por la integral.
Los siguiente ejemplos, triviales, pueden ayudarnos más adelante:

```{r integral-normalizada}
uniforme |> summarize(
  integral = PREC * sum(densidad),
  integral = integral / integral # Normalización
)

normal |> summarize(
  integral = PREC * sum(densidad),
  integral = integral / integral # Normalización
)
```

## Práctica

Calcula o comprueba las siguientes respuestas usando comandos de R:

### Pregunta 1

-   ¿Cuál es el valor máximo de la función de densidad de la distribución normal?

::: {#respuesta-1 .callout-note}
```{r}
#Para la distribución uniforme, el máximo está en cualquier valor y es simepre 1 o el valor máximo que se asigne.

dunif(0.5, min = 0, max = 1)

#Para la distribución normal, el máximo está en la media, que coincide con la moda:

dnorm(0, mean = 0, sd = 1)

#En este caso, el valor máximo que alcanza la distribución es 0,399 (aprox 0,400)
```
:::

### Pregunta 2

-   ¿Para qué valor de la variable aleatoria se da? ¿Cómo llamarías a ese valor?

::: {#respuesta-2 .callout-note}
#En ambos casos, el valor de frencuencia máxima es la moda.En el caso de la uniforme, todo el rango tiene el mismo valor, así que no hay una moda estricta como tal. Para la normal, el valor de la moda y el de la media coinciden, por lo que tanto media como moda serán x=0.
:::

### Pregunta 3

-   El valor máximo, ¿puede ser mayor que 1? Justifica tu respuesta.

::: {#respuesta-3 .callout-note}

#No, en la uniforme planteada, el 1 es el único valor posible (otra cosa sería que se redujese el rango, en cuyo caso, si el área no varía, podría ser mayor que 1). En cuanto a la normal, esta tendrá valores mayores que uno cuando la varianza adquiera valores muy bajos ya que el área bajo la curva debe ser siempre 1. Cabe destacar que el eje Y representa densidad, no probabilidad, por lo que sus valores pueden ser mayores a 1 sin problema.
:::

### Pregunta 4

-   Calcula la función de distribución de la variable normal **a partir de los valores de la función de densidad obtenidos previamente**, y represéntala.

*(Ejecuta `?cumsum` para consultar la ayuda de esa función).*

::: {#respuesta-4 .callout-note}
```{r}
# Se crean los datos para la CDF de una normal estándar (mu=0 y sigma=1)

normal_cdf <- tibble(
  variable = seq(-4, 4, by = PREC),
  cdf = pnorm(variable, mean = 0, sd = 1)
)

# Graficar la CDF
ggplot(normal_cdf, aes(x = variable, y = cdf)) +
  geom_line(color = PALETA[1], size = 1) +
  labs(title = "Función de Distribución Acumulativa (CDF) de la Normal Estándar",
       x = "Valor",
       y = "Probabilidad Acumulada")

# Calcular la función de densidad de probabilidad (PDF)
normal_pdf <- tibble(
  variable = seq(-4, 4, length.out = 1000),
  pdf = dnorm(variable, mean = 0, sd = 1)
)

# Graficar la PDF
ggplot(normal_pdf, aes(x = variable, y = pdf)) +
  geom_line(color = PALETA[2], size = 1) +
  labs(title = "Función de Densidad de Probabilidad (PDF) de la Normal Estándar",
       x = "Valor",
       y = "Densidad de Probabilidad")
```


:::

### Pregunta 5

-   Calcula el valor esperado de la distribución normal.

::: {#respuesta-5 .callout-note}
```{r}
#El valor esperado es la esperanza o media:
x_valsn <- seq(-4, 4, by = PREC)  
densidad <- dnorm(x_valsn, mean = 0, sd = 1)
mean_normal <- sum(x_valsn * densidad * PREC) / sum(densidad * PREC)
print(mean_normal)
#El valor esperado es virtualmente 0
```
:::

# Ejercicio 2

## Distribución Beta

### Pregunta 6

-   Representa una distribución Beta con parámetros $\alpha$ = $\beta$ = 1, $Beta(1, 1)$. Ajusta los ejes correctamente, si hace falta, como en la distribución uniforme.

*(Si no sabes qué limites utilizar, consulta la ayuda de `dbeta()`).*

::: {#respuesta-6 .callout-note}
```{r}
#Se crean los valores para beta

beta_1_1 <- tibble(
  variable = seq(0, 1, by = 1e-3),
  densidad = dbeta(variable, shape1 = 1, shape2 = 1)
)

#Gráficamente
ggplot(beta_1_1, aes(x = variable, y = densidad)) +
  geom_line(color = color_defecto) +
  ylim(0,1)
```
:::

### Pregunta 7

-   ¿Qué forma tiene?

::: {#respuesta-7 .callout-note}

#Esta función es no informativa, ya que es uniforme en [0,1].
:::

## Parámetros de la distribución Beta

### Pregunta 8

-   Prueba con diferentes valores de $\alpha$ y $\beta$.

::: {#respuesta-8 .callout-note}
```{r}
# Crear los datos con nuevos valores de alfa y beta
data_beta <- tibble(
  x = rep(seq(0, 1, by = 1e-3), 4),
  y = c(dbeta(seq(0, 1, by = 1e-3), 2, 2),
        dbeta(seq(0, 1, by = 1e-3), 5, 5),
        dbeta(seq(0, 1, by = 1e-3), 0.5, 0.5),
        dbeta(seq(0, 1, by = 1e-3), 1, 1)),  # Nueva curva con α=1, β=1
  curva = rep(c("α=2, β=2", "α=5, β=5", "α=0.5, β=0.5", "α=1, β=1"), each = length(seq(0, 1, by = 1e-3)))
)

# Gráfico
ggplot(data_beta, aes(x = x, y = y, color = curva)) +
  geom_line() +
  labs(title = "Distribuciones Beta con Diferentes Parámetros",
       x = "x",
       y = "Densidad",
       color = "Parámetros (α, β)") +  # Etiqueta de la leyenda
  theme_minimal()
```
:::


### Pregunta 9

-   ¿Qué ocurre a medida que van creciendo?

::: {#respuesta-9 .callout-note}

#Se observa que, cuando a y b son pequeños, la curva acumula densidad en los extremos, mientras que cuando crecen lo hacen el centro, asemejándose a una normal. Esto no es exactamente el TLC, pero es consistente con la propiedad de que una Beta con parámetros grandes concentra densidad en torno a la media.
:::

### Pregunta 10

-   ¿Qué ocurre cuando son iguales? ¿Y cuándo son distintos?

::: {#respuesta-10 .callout-note}
```{r}
# Crear los datos con diferentes valores de alfa y beta
data_beta <- tibble(
  x = rep(seq(0, 1, by = 1e-3), 5),
  y = c(dbeta(seq(0, 1, by = 1e-3), 1, 1),   # Uniforme
        dbeta(seq(0, 1, by = 1e-3), 5, 5),   # Simétrica concentrada en el centro
        dbeta(seq(0, 1, by = 1e-3), 0.5, 0.5), # Forma en "U"
        dbeta(seq(0, 1, by = 1e-3), 2, 5),   # Asimétrica a la izquierda
        dbeta(seq(0, 1, by = 1e-3), 5, 2)),  # Asimétrica a la derecha
  curva = rep(c("α=1, β=1 (Uniforme)", 
                "α=5, β=5 (Simétrica centrada (similar a normal))", 
                "α=0.5, β=0.5 (Densidad en extremos)", 
                "α=2, β=5 (Asimétrica a la izquierda)", 
                "α=5, β=2 (Asimétrica a la derecha)"), 
              each = length(seq(0, 1, by = 1e-3)))
)

# Graficar con leyenda
ggplot(data_beta, aes(x = x, y = y, color = curva)) +
  geom_line() +
  labs(title = "Distribuciones Beta con Diferentes Parámetros",
       x = "x",
       y = "Densidad",
       color = "Parámetros (α, β)")  # Etiqueta de la leyenda
 data_beta <- tibble(
  x = rep(seq(0, 1, by = 1e-3), 5),
  y = c(dbeta(seq(0, 1, by = 1e-3), 1, 1),   # α=1, β=1
        dbeta(seq(0, 1, by = 1e-3), 5, 5),   # α=5, β=5
        dbeta(seq(0, 1, by = 1e-3), 0.5, 0.5), # α=0.5, β=0.5
        dbeta(seq(0, 1, by = 1e-3), 2, 0.5), # α=2, β=0.5
        dbeta(seq(0, 1, by = 1e-3), 0.5, 2)), # α=0.5, β=2
  curva = rep(c("α=1, β=1", "α=5, β=5", "α=0.5, β=0.5", "α=2, β=0.5", "α=0.5, β=2"), 
              each = length(seq(0, 1, by = 1e-3)))
)


#Se observa que, cuando alfa=beta, hay simetría; cuando alfa<beta, hay asimetría a la izquierda; y cuando alfa>beta, a la derecha.
```
:::

### Pregunta 11

-   ¿Qué ocurre si tienen valores ligeramente superiores a 1?

::: {#respuesta-11 .callout-note}
```{r}
# Crear los datos con los nuevos valores de α y β
data_beta <- tibble(
  x = rep(seq(0, 1, by = 1e-3), 3),
  y = c(dbeta(seq(0, 1, by = 1e-3), 1.05, 1.05),  # Casi uniforme
        dbeta(seq(0, 1, by = 1e-3), 2, 1.05),    # Asimetría a la derecha
        dbeta(seq(0, 1, by = 1e-3), 1.05, 2)),   # Asimetría a la izquierda
  curva = rep(c("α=1.05, β=1.05 (Casi uniforme)", 
                "α=2, β=1.05 (Asimetría a la derecha)", 
                "α=1.05, β=2 (Asimetría a la izquierda)"), 
              each = length(seq(0, 1, by = 1e-3)))
)

# Graficar con leyenda
ggplot(data_beta, aes(x = x, y = y, color = curva)) +
  geom_line() +
  labs(title = "Distribuciones Beta con Diferentes Parámetros",
       x = "x",
       y = "Densidad",
       color = "Parámetros (α, β)") +  # Etiqueta de la leyenda
  theme_minimal()

#Cuando alfa=beta=ligeramente>1, se ve que la distribución empieza a tomar la forma acamapanada. Cuando solo uno de los parámtros es ligeramente>1, se observa que la asimetría de la función de exagera.
```
:::

### Pregunta 12

-   ¿Qué ocurre si tienen valores por debajo de 1?

::: {#respuesta-12 .callout-note}

```{r}
# Crear los datos con valores de alfa y beta ligeramente y moderadamente por debajo de 1
data_beta <- tibble(
  x = rep(seq(0, 1, by = 1e-3), 4),
  y = c(dbeta(seq(0, 1, by = 1e-3), 0.9, 0.9),   # Forma en "U"
        dbeta(seq(0, 1, by = 1e-3), 0.5, 0.5),   # Forma en "U" más pronunciada
        dbeta(seq(0, 1, by = 1e-3), 0.8, 0.5),   # Asimetría a la izquierda
        dbeta(seq(0, 1, by = 1e-3), 0.5, 0.8)),  # Asimetría a la derecha
  curva = rep(c("α=0.9, β=0.9 (Forma en U)", 
                "α=0.5, β=0.5 (Forma en U más pronunciada)", 
                "α=0.8, β=0.5 (Asimetría a la izquierda)", 
                "α=0.5, β=0.8 (Asimetría a la derecha)"), 
              each = length(seq(0, 1, by = 1e-3)))
)

# Graficar con leyenda
ggplot(data_beta, aes(x = x, y = y, color = curva)) +
  geom_line() +
  labs(title = "Distribuciones Beta con α y β < 1",
       x = "x",
       y = "Densidad",
       color = "Parámetros (α, β)") +  # Etiqueta de la leyenda
  theme_minimal()

# La curva toma la forma de U. La dirección del sesgo cuando alfa=/beta se invierte respecto a los valores >1.
```

```{r}
# Crear los datos con un alfa mayor que 1 y un beta menor que 1, y viceversa
data_beta <- tibble(
  x = rep(seq(0, 1, by = 1e-3), 2),
  y = c(dbeta(seq(0, 1, by = 1e-3), 2, 0.5),   # α > 1, β < 1 (Asimetría a la derecha)
        dbeta(seq(0, 1, by = 1e-3), 0.5, 2)),   # α < 1, β > 1 (Asimetría a la izquierda)
  curva = rep(c("α=2, β=0.5 (Asimetría a la derecha)", 
                "α=0.5, β=2 (Asimetría a la izquierda)"), 
              each = length(seq(0, 1, by = 1e-3)))
)

# Graficar con leyenda
ggplot(data_beta, aes(x = x, y = y, color = curva)) +
  geom_line() +
  labs(title = "Distribuciones Beta con α > 1 y β < 1, y α < 1 y β > 1",
       x = "x",
       y = "Densidad",
       color = "Parámetros (α, β)") +  # Etiqueta de la leyenda
  theme_minimal()

#Cuando solo un parámetro es <1, la U "pierde" uno de sus extremos.
```
:::

# Ejercicio 3

*(NOTA: Para todas las distribuciones, utiliza el valor de `PREC` definido en el ejercicio 1.)*

## Modelo beta-binomial

En el departamento de investigación de mercado de tu empresa quieren saber la tasa de aceptación de la nueva app que quieren lanzar.
Para ello, han probado la app con una muestra (asume m.a.s.) de $n$ potenciales usuarios/as, y se les ha pedido que indiquen si descargarían o no la app.

El jefe del departamento de analítica te asigna al proyecto y te pide que ajustes un modelo beta-binomial "no informativo" para responder a la pregunta de investigación.

### Pregunta 13

-   ¿Cómo se representa la "tasa de aceptación" en el modelo?

::: {#respuesta-13 .callout-note}

#La tasa de aceptación se representa como un parámetro de probabilidad, theta, que indica la proporción de usuarios que descargarían la app.
```{r}
# Definimos el parámetro de la tasa de aceptación
theta <- seq(0, 1, by = PREC)
```
:::

### Pregunta 14

-   ¿Qué distribución previa utilizarías para esa tasa de aceptación? Formúlala y represéntala gráficamente.

*(Ajusta los ejes correctamente, si hace falta, como en la distribución uniforme).*

::: {#respuesta-14 .callout-note}
```{r}
# La distribución no informativa será una beta(1,1), que es uniforme en [0,1].

# Distribución Beta(1,1) no informativa
prior <- dbeta(theta, shape1 = 1, shape2 = 1)

# Gráfico de la distribución previa
ggplot(data.frame(theta, prior), aes(x = theta, y = prior)) +
  geom_line(color = "blue", size = 1) +
  labs(title = "Distribución Previa Beta(1,1)", x = "Tasa de aceptación (θ)", y = "Densidad")+
  ylim(0, 1)
```
:::

### Pregunta 15

-   Supón que $y$ es el número de usuarios/as que han respondido que "Sí" descargarían la app. Formula la verosimilitud del modelo.

::: {#respuesta-15 .callout-note}
```{r}
# La verosimilitud del modelo sigue una distribución binomial, ya que es la respuesta binaria del número de personas que descargarían la app.

# Función de verosimilitud binomial
likelihood <- function(theta, y, n) {
  return(dbinom(y, size = n, prob = theta))
}

plot(1, type="n", axes=FALSE, xlab="", ylab="") # Crea un lienzo vacío
text(1, 1, expression(L(theta ~ "|" ~ y ~ "," ~ n) == C(n, y) * theta^y * (1 - theta)^(n - y)), cex=1.5)


```

## Ajuste del modelo

-   El departamento de investigación de mercado te da acceso a los siguientes datos de la muestra:

```{r beta-binomial-muestra}
aceptacion_muestra <- tibble(
  id_participante   = 1:22,
  resp_descarga_app = c(
    "Si", "Si", "No", "No", "Si", "Si", "Si", "Si", "No", "Si", "Si",
    "Si", "Si", "Si", "Si", "Si", "No", "Si", "No", "Si", "Si", "Si"
  )
)
```

```{r}
#El modelo quedaría como:

# Función de verosimilitud binomial
likelihood <- function(theta, y, n) {
  return(dbinom(y, size = n, prob = theta))
}

# Datos de la muestra
n <- aceptacion_muestra |> nrow()
y <- sum(aceptacion_muestra$resp_descarga_app == "Si")

# Verosimilitud para distintos valores de theta
theta_values <- seq(0, 1, length = 100)
likelihood_values <- sapply(theta_values, likelihood, y = y, n = n)

# Graficamos la verosimilitud
ggplot(data.frame(theta = theta_values, likelihood = likelihood_values), aes(x = theta, y = likelihood)) +
  geom_line(color = "purple", size = 1) +
  labs(title = "Verosimilitud de la tasa de aceptación (θ)", x = "Tasa de aceptación (θ)", y = "Verosimilitud") +
  theme_minimal()
```


:::

### Pregunta 16

-   Obtén, en base a estos datos, la distribución posterior de la tasa de aceptación (en forma analítica), y represéntala junto a la distribución previa.

::: {#respuesta-16 .callout-note}
```{r}

# Contar cuántos "Sí" y "No" hay
y <- sum(aceptacion_muestra$resp_descarga_app == "Si")
n <- nrow(aceptacion_muestra)

# Parámetros de la distribución previa (Beta(1,1))
alpha_prior <- 1
beta_prior <- 1

# Parámetros de la distribución posterior de acuerdo a la definición para el caso
alpha_post <- alpha_prior + y
beta_post <- beta_prior + n - y


# Crear un data frame para graficar
theta_values <- seq(0, 1, length.out = 1000)
prior_values <- dbeta(theta_values, alpha_prior, beta_prior)
posterior_values <- dbeta(theta_values, alpha_post, beta_post)

# Graficar
df <- tibble(theta = theta_values, Prior = prior_values, Posterior = posterior_values)

ggplot(df, aes(x = theta)) +
  geom_line(aes(y = Prior, color = "Previo"), size = 1, linetype = "dashed") +
  geom_line(aes(y = Posterior, color = "Posterior"), size = 1) +
  labs(title = "Distribuciones Beta: Previa vs Posterior",
       x = "Tasa de aceptación",
       y = "Densidad") +
  theme_minimal() +
  theme(legend.position = "top") +
  scale_color_manual(values = c("Previo" = PALETA[1], "Posterior" = PALETA[2]))
```

:::

### Pregunta 17

-   Obtén por el método numérico el valor esperado y la moda de la distribución posterior. ¿Cómo los interpretarías?

*(Nota: Ten en cuenta la "precisión" al calcular el "peso" de cada muestra.)*

::: {#respuesta-17 .callout-note}
```{r}
# Valor esperado de la distribución posterior
expected_value <- alpha_post / (alpha_post + beta_post)

# Moda de la distribución posterior (siempre que alpha > 1 y beta > 1, por la forma de la distribución)

if (alpha_post > 1 && beta_post > 1) {
  mode_value <- (alpha_post - 1) / (alpha_post + beta_post - 2)
} else {
  # Si no se puede calcular la moda, la moda será un valor en los extremos
  mode_value <- ifelse(alpha_post > beta_post, 1, 0)
}

# Mostrar resultados
expected_value
#Muestra el valor de la probabilidad media estimada de éxito, ponderada por la prior y los datos observados

mode_value
#Muestra el valor de theta más probable en la distribución posterior.
```
:::

## Ajuste con una nueva muestra

-   El director de investigación de mercado no está totalmente seguro con los resultados, y pide a su departamento recoger una nueva muestra, mayor, para el estudio. Te dan acceso a los siguientes datos de la nueva muestra:

```{r beta-binomial-muestra2}
aceptacion_muestra_2 <- tibble(
  id_participante   = 1:113,
  resp_descarga_app = c(
    "Si", "Si", "No", "No", "Si", "Si", "Si", "Si", "No", "Si", "Si",
    "Si", "Si", "Si", "Si", "Si", "No", "Si", "No", "Si", "Si", "Si", 
    "No", "Si", "Si", "Si", "Si", "No", "No", "Si", "No", "Si", "Si", 
    "Si", "Si", "Si", "No", "Si", "No", "No", "Si", "No", "Si", "Si", 
    "No", "No", "No", "Si", "No", "No", "Si", "Si", "No", "No", "Si", 
    "No", "Si", "No", "No", "No", "Si", "Si", "No", "Si", "Si", "No", 
    "Si", "Si", "No", "Si", "Si", "No", "Si", "No", "Si", "No", "Si", 
    "No", "No", "No", "Si", "Si", "No", "No", "Si", "Si", "No", "No", 
    "No", "Si", "Si", "No", "Si", "Si", "No", "Si", "Si", "Si", "Si", 
    "No", "Si", "No", "No", "No", "No", "No", "Si", "No", "No", "Si", 
    "Si", "Si", "Si"
  )
)
```

### Pregunta 18

-   ¿Qué distribución previa utilizarías en esta ocasión? Formúlala.

::: {#respuesta-18 .callout-note}
```{r}
# Se podría emplear la posterior del anterior ejercicio como prior de este, ya que se actualizaría la información sobre la misma.

# Datos de la primera muestra (ya conocidos)
y1 <- sum(aceptacion_muestra$resp_descarga_app == "Si")
n1 <- nrow(aceptacion_muestra)

# Parámetros posteriores de la primera muestra
alpha_posterior_1 <- 1 + y1
beta_posterior_1 <- 1 + n1 - y1

# Usar la posterior como prior para la nueva muestra
prior_alpha <- alpha_posterior_1
prior_beta <- beta_posterior_1

# Representar la distribución Beta como prior
curve(dbeta(x, prior_alpha, prior_beta), from = 0, to = 1, 
      main = "Distribución previa (Posterior de la muestra anterior)",
      ylab = "Densidad", xlab = "Tasa de aceptación")
```
:::

### Pregunta 19

-   Obtén la distribución posterior analítica después de esta segunda muestra, represéntala junto con las dos distribuciones anteriores, y obtén los estimadores posteriores esperado y modal usando el método numérico.

::: {#respuesta-19 .callout-note}
```{r}

library(ggplot2)

# Número de respuestas "Sí" en la primera muestra
y1 <- sum(aceptacion_muestra$resp_descarga_app == "Si")
n1 <- nrow(aceptacion_muestra)

# Número de respuestas "Sí" en la segunda muestra
y2 <- sum(aceptacion_muestra_2$resp_descarga_app == "Si")
n2 <- nrow(aceptacion_muestra_2)

# Parámetros de la distribución posterior después de la primera muestra
alpha_1 <- 1 + y1
beta_1 <- 1 + n1 - y1

# Actualización de los parámetros con la segunda muestra
alpha_2 <- alpha_1 + y2
beta_2 <- beta_1 + n2 - y2

# Rango de x
x_range <- seq(0, 1, by=PREC)

# Crear un dataframe con todas las distribuciones
data <- data.frame(
  x = rep(x_range, 3),
  y = c(dbeta(x_range, 1, 1), 
        dbeta(x_range, alpha_1, beta_1),
        dbeta(x_range, alpha_2, beta_2)),
  Distribución = factor(rep(c("Previo (Beta(1,1))", 
                              "Posterior 1ª Muestra", 
                              "Posterior 2ª Muestra"), 
                            each = length(x_range)))
)

# Graficar con ggplot2
ggplot(data, aes(x = x, y = y, color = Distribución)) +
  geom_line(size = 1) +
  scale_color_manual(values = c("gray", "red", "blue")) +
  labs(title = "Distribuciones Previa y Posteriores",
       x = "Tasa de Aceptación",
       y = "Densidad") +
  theme_minimal()

# Estimadores posteriores
esperado <- alpha_2 / (alpha_2 + beta_2)
modal <- ifelse(alpha_2 > 1 & beta_2 > 1, 
                (alpha_2 - 1) / (alpha_2 + beta_2 - 2), NA)

# Mostrar resultados
list(esperado = esperado, modal = modal)

```
:::

## Ajuste con las muestras colapsadas

Supón que el director de investigación de mercado no estaba contento con la muestra inicial y pidió recoger más muestra antes de darte acceso a los datos.
Cuando recibes los datos, recibes las dos muestras colapsadas, sin saber qué participantes eran de la primera o de la segunda muestra:

```{r beta-binomial-muestra-total}
aceptacion_muestra_total <- bind_rows(
  aceptacion_muestra, aceptacion_muestra_2
) |>
  mutate(id_participante = row_number()) # Los ID están colapsados en una serie
```

### Pregunta 20

-   Obtén la distribución posterior analítica después de esta segunda muestra, represéntala junto con las distribuciones anteriores, y obtén los estimadores posteriores esperado y modal por el método numérico.

::: {#respuesta-20 .callout-note}
```{r}
# Calcular el número de respuestas "Sí" y el tamaño total
y_total <- sum(aceptacion_muestra_total$resp_descarga_app == "Si")
n_total <- nrow(aceptacion_muestra_total)

# Parámetros de la distribución posterior total
alpha_total <- 1 + y_total
beta_total <- 1 + n_total - y_total

# Rango para el eje x
x_range <- seq(0, 1, length.out = 100)

# Obtener el máximo valor de la densidad de las distribuciones para ajustar el eje y
max_y <- max(c(max(dbeta(x_range, 1, 1)), max(dbeta(x_range, alpha_1, beta_1)),
                max(dbeta(x_range, alpha_2, beta_2)), max(dbeta(x_range, alpha_total, beta_total))))

# Graficar distribuciones
plot(x_range, dbeta(x_range, 1, 1), type = "l", col = "gray", lwd = 2, 
     main = "Distribuciones Previa y Posteriores", 
     ylab = "Densidad", xlab = "Tasa de Aceptación", 
     xlim = c(0, 1), ylim = c(0, max_y * 1.1)) # Ajuste del eje y

lines(x_range, dbeta(x_range, alpha_1, beta_1), col = "red", lwd = 2)
lines(x_range, dbeta(x_range, alpha_2, beta_2), col = "blue", lwd = 2)
lines(x_range, dbeta(x_range, alpha_total, beta_total), col = "green", lwd = 2)

# Leyenda
legend("topleft", legend = c("Previo", "Posterior 1ª Muestra", "Posterior 2ª Muestra", "Posterior Total"),
       col = c("gray", "red", "blue", "green"), lwd = 2)

# Estimadores posterior
esperado_total <- alpha_total / (alpha_total + beta_total)
modal_total <- ifelse(alpha_total > 1 & beta_total > 1, (alpha_total - 1) / (alpha_total + beta_total - 2), NA)

# Resultados
list(esperado = esperado_total, modal = modal_total)
```
:::

### Pregunta 21

-   ¿Qué concluyes de la respuesta anterior? ¿En qué se diferencia este enfoque del análisis de datos clásico o frecuentista?

::: {#respuesta-21 .callout-note}
# La conclusión es que el integrar los datos secuancialmente o colapsados lleva a las mismas conclusiones ya que, el Análisis Bayesiano, a diferencia del Frecuantista, permite una actualización de parámetros ya estimados cuando se tiene acceso a más información.
:::

# Ejercicio 4

*(NOTA: Para todas las distribuciones, utiliza el valor de `PREC` definido en el ejercicio 1.)*

En un proyecto de investigación educativo, el equipo investigador ha evaluado la rapidez de lectura en las dos clases de 1º de ESO de un colegio.
Los datos que te entregan consisten en el tiempo en segundos que tarda cada niño en leer un texto estandarizado.

Se quiere obtener un parámetro global promedio del tiempo de lectura para el alumnado de 1º de ESO en el colegio, para lo que te piden ajustar un modelo normal-normal.
Se pide usar como distribución previa la estimada de la población, que tiene media y varianza de 247 y 1156, respectivamente.

Los datos que te han facilitado son:

```{r normal-normal-muestras}
clase_1 <- tibble(
  id     = 1:27,
  tiempo = c(
    242, 249, 278, 273, 227, 257, 276, 236, 214, 141, 200, 201, 
    228, 271, 160, 275, 156, 246, 293, 306, 263, 247, 224, 160, 277, 
    168, 250
  )
)

clase_2 <- tibble(
  id     = 1:24,
  tiempo = c(
    195, 176, 237, 258, 226, 254, 292, 212, 215, 298, 235, 244, 
    144, 227, 166, 194, 261, 187, 224, 233, 180, 167, 193, 282
  )
)
```

## Modelo normal-normal

### Pregunta 22

-   Determina la verosimilitud y las distribuciones previa y posterior de la media, asumiendo que la varianza de la verosimilitud es la varianza de los datos. Justifica cómo has obtenido los parámetros de la distribución posterior (usa 2 decimales de precisión).

::: {#respuesta-22 .callout-note}
```{r}
#Cálculo de las medias muestrales
mean_clase_1 <- mean(clase_1$tiempo)
mean_clase_2 <- mean(clase_2$tiempo)

# Número de observaciones en cada clase
n_clase_1 <- length(clase_1$tiempo)
n_clase_2 <- length(clase_2$tiempo)

#Cálculo de los parámetros conjuntos
mean_total <- (mean_clase_1 * n_clase_1 + mean_clase_2 * n_clase_2) / (n_clase_1 + n_clase_2)
n_total <- n_clase_1 + n_clase_2

# Parámetros de la distribución previa
mu_0 <- 247   # Media de la distribución previa
sigma_0_sq <- 1156  # Varianza de la distribución previa

# Parámetros de la verosimilitud
# Usamos la varianza de la población para actualizarla (reducirla)
sigma_sq <- sigma_0_sq  

# Cálculo de la media y varianza de la distribución posterior
# Ambos parámetros se definen por:
plot(1, type="n", axes=FALSE, xlab="", ylab="", xlim=c(0, 10), ylim=c(0, 10))
text(5, 9, expression(mu[post] == frac(mu[0], sigma[0]^2) + frac(bar(x)[1] * n[1], sigma^2) / (frac(1, sigma[0]^2) + frac(n[1], sigma^2))), cex=1.2)
text(5, 6, expression(sigma[post]^2 == frac(1, frac(1, sigma[0]^2) + frac(n[1], sigma^2))), cex=1.2)
title("Fórmulas Posteriores")

# Media posterior: La media posterior se obtiene mediante una combinación ponderada de la media de la distribución previa y la media de los datos observados donde los pesos dependen de la varianza de cada fuente de información.

mu_post <- (mu_0 / sigma_0_sq + mean_total * n_total / sigma_sq) / 
           (1 / sigma_0_sq + n_total / sigma_sq)

# Varianza posterior
#La varianza posterior se ve estimada por la siguiente fórmula, esperandos siempre que se reduzca al incorporar más información.
sigma_post_sq <- 1 / (1 / sigma_0_sq + n_total/ sigma_sq)

# Mostrar los resultados
cat("La media posterior es:", round(mu_post, 2), "\n")
cat("La varianza posterior es:", round(sigma_post_sq, 2), "\n")

```
:::



### Pregunta 23

-   Representa las distribuciones previa y posterior de la media; considera un eje que cubra 4 desviaciones típicas a cada lado de la media de la distribución previa. Obten el estimador esperado y modal a partir de esta distribución y compáralos con la solución analítica de la pregunta anterior.

::: {#respuesta-23 .callout-note}
```{r}
#La media vendrá definida por esta ecuación
plot(1, type="n", axes=FALSE, xlab="", ylab="", xlim=c(0, 10), ylim=c(0, 10))
text(5, 7, expression(E(mu) == integral(x * p(x) * dx)), cex=1.5)
title("Cálculo de la Media Posterior")

# Generar la secuencia de x_vals
x_vals <- seq(mu_0 - 4 * sqrt(sigma_0_sq), mu_0 + 4 * sqrt(sigma_0_sq), by=PREC)

# Distribución previa
prior <- dnorm(x_vals, mean = mu_0, sd = sqrt(sigma_0_sq))

# Cálculo de la verosimilitud
mean_total <- (mean_clase_1 * n_clase_1 + mean_clase_2 * n_clase_2) / (n_clase_1 + n_clase_2)
n_total <- n_clase_1 + n_clase_2  # Número total de observaciones
likelihood <- dnorm(x_vals, mean = mean_total, sd = sqrt(sigma_sq / n_total))

# Distribución posterior
posterior <- prior * likelihood  

# df con los valores de la distribución posterior
normal <- data.frame(x = x_vals, densidad = posterior)

# "Normalizar" la distribución posterior (que el área sea 1)
integral_total <- normal |> summarize(integral = sum(densidad) * PREC) |> pull(integral)
normal <- normal |> mutate(densidad = densidad / integral_total)

# Cálculo de la media posterior por integración numérica
mu_post_integrado <- normal |> summarize(media_post = sum(x * densidad) * PREC) |> pull(media_post)

# Cálculo de la varianza posterior por integración numérica
sigma_post_sq_integrado <- normal |> summarize(var_post = sum((x - mu_post_integrado)^2 * densidad) * PREC) |> pull(var_post)

# Cálculo de la moda posterior (el valor de x donde la posterior es máxima)
estimador_modal <- normal |> filter(densidad == max(densidad)) |> pull(x)

# Representación gráfica
plot(normal$x, prior, type = "l", col = "blue", lwd = 2, 
     main = "Distribución Previa y Posterior de la Media", 
     xlab = "Media de Tiempo de Lectura", ylab = "Densidad",
     xlim = c(mu_0 - 4 * sqrt(sigma_0_sq), mu_0 + 4 * sqrt(sigma_0_sq)),
     ylim = c(0, max(normal$densidad) + 0.01))

lines(normal$x, normal$densidad, col = "red", lwd = 2)
abline(v = mu_post_integrado, col = "green", lwd = 2, lty = 2)  # Media posterior
abline(v = estimador_modal, col = "purple", lwd = 2, lty = 2)  # Moda posterior

legend("topright", legend = c("Distribución Previa", "Distribución Posterior", 
                              "Media Posterior (Integrada)", "Moda Posterior"),
       col = c("blue", "red", "green", "purple"), lwd = c(2, 2, 2, 2), lty = c(1, 1, 2, 2))

# Imprimir valores estimados
cat("Estimador esperado (media posterior integrada):", round(mu_post_integrado, 2), "\n")
cat("Varianza posterior integrada:", round(sigma_post_sq_integrado, 2), "\n")
cat("Estimador modal (moda posterior):", round(estimador_modal, 2), "\n")


#Los valores coinciden con la solución analítica. La discrepancia mínima media-moda puede deberse a efectos del cálculo.
```
::: 
